{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Innoplexus_sentiment_analysis_v0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N1Abccn5un9",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis for drugs/medicines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3U244Urcd4u",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Here are the steps for sentiment analysis\n",
        "1.   Cleaning the text\n",
        "2.   Creating word embeddings\n",
        "3.   Training the network\n",
        "4.   Final prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCWAFS8b6TSc",
        "colab_type": "text"
      },
      "source": [
        "Installing the necessary packages like tensorflow-gpu,emoji etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmP_E7Zj6SZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "c8b5fb45-2ea6-448c-8d07-adad08c0ef9a"
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 45kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.4)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.16.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (0.15.5)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zExeT-T969XN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bbcde820-9106-4f71-a1f7-65963ebaa8e7"
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNBKjqx57Hm2",
        "colab_type": "text"
      },
      "source": [
        "Now mount the drive to load the training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-376i0j37G3L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "97ae2324-fee8-4f17-cf22-31e653f8c9f2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yifMjVAG7kce",
        "colab_type": "text"
      },
      "source": [
        "Importing the packges "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRXsGdBy5qS5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8f4a7c83-888d-4cce-9f22-f8702d7a2f43"
      },
      "source": [
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np # linear algebra\n",
        "import emoji # handling emoticons in the text\n",
        "import nltk # tokenizing\n",
        "import string # string manipulation\n",
        "import gensim # word embeddings\n",
        "import tensorflow as tf\n",
        "from autocorrect import spell\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.corpus import stopwords \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV3a8AOy-E3T",
        "colab_type": "text"
      },
      "source": [
        "visualisation of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8mUaue08gtv",
        "colab_type": "code",
        "outputId": "709c22f4-a8e3-4dee-a193-0602bdf061ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/Innoplexus/train_F3WbcTw.csv')\n",
        "train.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>unique_hash</th>\n",
              "      <th>text</th>\n",
              "      <th>drug</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0</td>\n",
              "      <td>Autoimmune diseases tend to come in clusters. ...</td>\n",
              "      <td>gilenya</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9eba8f80e7e20f3a2f48685530748fbfa95943e4</td>\n",
              "      <td>I can completely understand why you’d want to ...</td>\n",
              "      <td>gilenya</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fe809672251f6bd0d986e00380f48d047c7e7b76</td>\n",
              "      <td>Interesting that it only targets S1P-1/5 recep...</td>\n",
              "      <td>fingolimod</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bd22104dfa9ec80db4099523e03fae7a52735eb6</td>\n",
              "      <td>Very interesting, grand merci. Now I wonder wh...</td>\n",
              "      <td>ocrevus</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b227688381f9b25e5b65109dd00f7f895e838249</td>\n",
              "      <td>Hi everybody, My latest MRI results for Brain ...</td>\n",
              "      <td>gilenya</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                unique_hash  ... sentiment\n",
              "0  2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0  ...         2\n",
              "1  9eba8f80e7e20f3a2f48685530748fbfa95943e4  ...         2\n",
              "2  fe809672251f6bd0d986e00380f48d047c7e7b76  ...         2\n",
              "3  bd22104dfa9ec80db4099523e03fae7a52735eb6  ...         2\n",
              "4  b227688381f9b25e5b65109dd00f7f895e838249  ...         1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vG9n7z6O8Yd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "e0cd2f00-149e-464d-d68b-7fd070cdf24b"
      },
      "source": [
        "print('Total number of samples',len(train.index))\n",
        "print(train['sentiment'].value_counts())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of samples 5279\n",
            "2    3825\n",
            "1     837\n",
            "0     617\n",
            "Name: sentiment, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y709I32Y_gVt",
        "colab_type": "text"
      },
      "source": [
        "**STEP 1: Cleaning the text**\n",
        "\n",
        "This process can be furture divided into smaller steps as follows \n",
        "\n",
        "1.   Demojize the sentences\n",
        "2.   Removing punctuations\n",
        "3.   Conevrting sentences to lower case \n",
        "4.   Tokenizing the words\n",
        "5.   Removing the stop words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB3puKxFA-qM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "1c2fa173-9fe5-497d-c1bb-4456fc65b792"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU_rCsal-ztu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process(list_sent): \n",
        "  # This function is used for text cleaning \n",
        "  ps = PorterStemmer()\n",
        "  de_emoji = [emoji.demojize(sent) for sent in list_sent]\n",
        "  rem_punc = [sent.translate(str.maketrans('','',string.punctuation)) for sent in de_emoji]\n",
        "  rem_num = [sent.translate(str.maketrans('','','0123456789')) for sent in rem_punc]\n",
        "  norm_corpus = [sent.lower() for sent in rem_num]\n",
        "  tok_corpus = [nltk.word_tokenize(sent) for sent in norm_corpus]\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "  filtered_corpus = []\n",
        "  for sent in tok_corpus:\n",
        "    filtered_sent = []\n",
        "    for word in sent:\n",
        "      word = ps.stem(word)\n",
        "      if not word in stop_words:\n",
        "        filtered_sent.append(word)\n",
        "    filtered_corpus.append(filtered_sent)\n",
        "  return filtered_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RwxMbN0B1vl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = train['text'].values.tolist()\n",
        "x_clean = pre_process(x)\n",
        "del x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uYuv_58CkvY",
        "colab_type": "text"
      },
      "source": [
        "**STEP 2: Creating word embeddings**\n",
        "\n",
        "Word Embedding is a representation of text where words that have the same meaning have a similar representation. In other words it represents words in a coordinate system where related words, based on a corpus of relationships, are placed closer together.\n",
        "\n",
        "Here we use gensim package which creates word embeddings for us"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy0GoEydQy3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTOR_SIZE = 100 # length of embedding vector\n",
        "model = gensim.models.Word2Vec(x_clean,min_count=1,size = 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5fBIYFgEd8O",
        "colab_type": "text"
      },
      "source": [
        "Below is an example showing word 'patients' is related to other words in the corpus "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XcLcMuFTWvi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "cab20b27-795d-409a-cfed-af6a2a4dd61b"
      },
      "source": [
        "model.most_similar('patient')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('individu', 0.7232431769371033),\n",
              " ('particip', 0.7034831047058105),\n",
              " ('popul', 0.7011430263519287),\n",
              " ('among', 0.6941633224487305),\n",
              " ('women', 0.6825122833251953),\n",
              " ('initi', 0.6628513336181641),\n",
              " ('observ', 0.6550769805908203),\n",
              " ('monotherapi', 0.6412683725357056),\n",
              " ('enrol', 0.6347674131393433),\n",
              " ('previous', 0.6340286135673523)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCTbz1pcU7xA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "e4589b61-5aea-4997-d910-a12c7a021a44"
      },
      "source": [
        "x_feature = [model[words] for words in x_clean]\n",
        "\n",
        "del x_clean"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WcojTkAFBV5",
        "colab_type": "text"
      },
      "source": [
        "convecting the unequal length sequence to equal length squences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC3T3gvVZ5ZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 500\n",
        "\n",
        "x_feature = pad_sequences(x_feature,maxlen=MAX_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKk-1RjiEZnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_labels = np.asarray(train['sentiment'].values.tolist())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSisjKNHF4dZ",
        "colab_type": "text"
      },
      "source": [
        "spliting the data into testing and training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Hvdkcn6Evu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(x_feature,y_labels, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DygulcFy2HEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del x_feature\n",
        "del y_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNQ8-Qkcys_I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "54e3a7fa-a67d-4e23-823f-562f06ae0b15"
      },
      "source": [
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "print(len(Y_train))\n",
        "print(len(Y_test))\n",
        "print(X_train.shape[1:])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4223\n",
            "1056\n",
            "4223\n",
            "1056\n",
            "(500, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43eTY16a1dUL",
        "colab_type": "text"
      },
      "source": [
        "**STEP 3: Training the model**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPGPscBTNzJz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "d2d9abc9-1b29-4fc4-c2e6-ebed83bde8a9"
      },
      "source": [
        "lstm_out1 = 200\n",
        "\n",
        "out_senti = 3\n",
        "analysis_model = Sequential()\n",
        "analysis_model.add(LSTM(lstm_out1,input_shape=(X_train.shape[1:]), dropout=0.2, recurrent_dropout=0.2))\n",
        "analysis_model.add(Dense(100,activation='relu'))\n",
        "analysis_model.add(Dense(3,activation='softmax'))\n",
        "analysis_model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "analysis_model.fit(X_train,Y_train,epochs=3,validation_data=(X_test,Y_test))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4223 samples, validate on 1056 samples\n",
            "Epoch 1/3\n",
            "4223/4223 [==============================] - 323s 77ms/sample - loss: 0.7842 - acc: 0.7218 - val_loss: 0.7194 - val_acc: 0.7292\n",
            "Epoch 2/3\n",
            "4223/4223 [==============================] - 323s 76ms/sample - loss: 0.7501 - acc: 0.7239 - val_loss: 0.7197 - val_acc: 0.7292\n",
            "Epoch 3/3\n",
            "4223/4223 [==============================] - 318s 75ms/sample - loss: 0.7343 - acc: 0.7234 - val_loss: 0.7178 - val_acc: 0.7292\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fab4fa3ca20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}